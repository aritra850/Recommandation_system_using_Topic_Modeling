{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ATvH-zYJgCpW",
    "outputId": "f3e1d14a-d578-4bf5-e733-2a8938c66173"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from bnlp import NLTKTokenizer\n",
    "from bnlp.corpus import stopwords, punctuations\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Akt5U8Kjgr76"
   },
   "outputs": [],
   "source": [
    "# READING DATASET\n",
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 808
    },
    "id": "kHlgO_EuhZMO",
    "outputId": "8bb2a308-8079-4d1c-b470-37f46086e5a7"
   },
   "outputs": [],
   "source": [
    "# VISUALISING DATASET\n",
    "pd.set_option(\"display.max_colwidth\",-1)\n",
    "df.drop(columns=[\"title\",\"label\"],inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6aN_kZPukCUn",
    "outputId": "f17763ba-b038-4689-f1d9-d6c6c1bb7913"
   },
   "outputs": [],
   "source": [
    "# REMOVING EMPTY ARTICLES\n",
    "df.dropna(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Gb9aNfVk-RW"
   },
   "outputs": [],
   "source": [
    "def todict(topics):\n",
    "  dict = {}\n",
    "  for j in range(len(topics)):\n",
    "    dict[topics[j][0]] = topics[j][1]\n",
    "  return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_alloc(dic,num):\n",
    "    for i in range(0,num):\n",
    "        if i not in dic.keys():\n",
    "            dic[i] = -1\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9y9B3M7Rxi0"
   },
   "outputs": [],
   "source": [
    "def foreign_removal(word):\n",
    "  li = []\n",
    "  num = ['১','২','৩','৪','৫','৬','৭','৮','৯','০']\n",
    "  for letter in word:\n",
    "    if 2433<=ord(letter)<=2554 and letter not in num:\n",
    "      li.append(letter)\n",
    "  return \"\".join(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJ_NMYuNhvkq"
   },
   "outputs": [],
   "source": [
    "tokenizer = NLTKTokenizer()\n",
    "def preprocess(text):\n",
    "  tokens = tokenizer.word_tokenize(text)\n",
    "  token_list = []\n",
    "  rem = [\"এক\",\"হয়\",\"হয়ে\",\"হয়েছে\",\"দিয়ে\",\"একটা\",\"যায়\"]\n",
    "  for token in tokens:\n",
    "    if len(foreign_removal(token))>0:\n",
    "      token_list.append(foreign_removal(token))\n",
    "  words = [word for word in token_list if word not in stopwords and word not in punctuations and word not in rem]\n",
    "  return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GKxAVwnghu5b",
    "outputId": "b23f0081-bc8c-4e48-f9e2-fd33129bd168"
   },
   "outputs": [],
   "source": [
    "# APPLYINGING PREPROCESSING ON DATASET\n",
    "tokenized_text = df[\"article\"].apply(preprocess)\n",
    "\n",
    "bigram_phrases = Phrases(tokenized_text,min_count = 5,threshold = 50)\n",
    "\n",
    "bigram = Phraser(bigram_phrases)\n",
    "\n",
    "def make_bigram(text):\n",
    "    return [bigram[word] for word in text]\n",
    "\n",
    "tokenize_text = make_bigram(tokenized_text)\n",
    "\n",
    "tokenize_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_H3Qg6V_hHLj",
    "outputId": "30cb0422-5494-4def-d26a-2ef8ae87520b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LDA MODEL TRAINING & FINDING BEST NUMBER OF TOPICS FOR THE DATASET\n",
    "scores = []\n",
    "model_list = []\n",
    "dictionary = corpora.Dictionary(tokenize_text)\n",
    "dtm = [dictionary.doc2bow(word) for word in tokenize_text]\n",
    "for i in range(5,21):\n",
    "  LDA_model = LdaModel(corpus=dtm,num_topics=i,id2word=dictionary,random_state=50)\n",
    "  coherence_model = CoherenceModel(model=LDA_model,texts=tokenize_text,dictionary=dictionary)\n",
    "  score = coherence_model.get_coherence()\n",
    "  scores.append(score)\n",
    "  model_list.append(LDA_model)\n",
    "  print(f\"COHERENCE SCORE FOR {i} TOPICS = {score}\")\n",
    "\n",
    "iter = np.argmax(scores)\n",
    "num = iter + 5\n",
    "\n",
    "lda = model_list[iter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 916
    },
    "id": "cjFE-QlRQr-O",
    "outputId": "3de65f5a-cf1f-4ef8-cfa8-bf1e3d2dc8f8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# USING PYLDAVIS ON THE TRAINED LDA MODEL\n",
    "gensimvis.prepare(lda,dtm,dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 840
    },
    "id": "WEFvpXwwzsPU",
    "outputId": "e11135b8-fa4f-4d79-c286-1cfaf7b7fbd4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ASSIGNING TOPIC TO THE DATASET\n",
    "\n",
    "topics = list(lda.get_document_topics(dtm))\n",
    "\n",
    "topic_doc = dict({})\n",
    "for i in range(num):\n",
    "    topic_doc[i] = []\n",
    "    \n",
    "for topic in topics:\n",
    "  topic_dist = topic_alloc(todict(topic),num)\n",
    "  for i in range(num):\n",
    "    topic_doc[i].append(topic_dist[i])\n",
    "\n",
    "print(topic_doc.keys())\n",
    "\n",
    "print(df.columns)\n",
    "for i in topic_doc.keys():\n",
    "    df[str(i)] = topic_doc[i]\n",
    "\n",
    "print(df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORTING NECESSARY FILES\n",
    "\n",
    "df.to_csv(\"bangla_news.csv\")\n",
    "\n",
    "filename = \"recom.pkl\"\n",
    "fobj = open(filename,\"wb\")\n",
    "pickle.dump(lda,fobj)\n",
    "fobj.close()\n",
    "\n",
    "fn = \"dtm.pkl\"\n",
    "f = open(fn,\"wb\")\n",
    "pickle.dump(list(tokenize_text),f)\n",
    "f.close()\n",
    "\n",
    "fd = \"dict.pkl\"\n",
    "fdo = open(fd,\"wb\")\n",
    "pickle.dump(dictionary,fdo)\n",
    "fdo.close()\n",
    "\n",
    "fbi = \"bigram.pkl\"\n",
    "fbio = open(fbio,\"wb\")\n",
    "pickle.dump(bigram,fbio)\n",
    "fbio.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "bangla_news.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
